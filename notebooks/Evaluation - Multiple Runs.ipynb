{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#Basic Imports\n",
    "import os,sys\n",
    "os.chdir(\"/home/asebaq/MSMatch\")\n",
    "\n",
    "from tqdm import tqdm,trange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report,confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "import pandas\n",
    "\n",
    "from datasets.ssl_dataset import SSL_Dataset\n",
    "from datasets.data_utils import get_data_loader\n",
    "from utils import get_model_checkpoints\n",
    "from utils import net_builder\n",
    "from utils import clean_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the runs to load\n",
    "csv_folder = \"/home/asebaq/MSMatch/cvs_folder\" #Path where tmp csv files are stored\n",
    "folder = \"/home/asebaq/MSMatch/trained_models/ms/50/fixmatch/eurosat_ms/FixMatch_archefficientnet-b0_batch64_confidence0.95_lr0.03_uratio7_wd0.0005_wu1.0_seed0_numlabels50_optSGD\" #Path to the runs to load \n",
    "sort_criterion = \"numlabels\" # Accepted net, numlabels\n",
    "seed_wanted = 1 # Seed wanted (the others will be filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home\n"
     ]
    }
   ],
   "source": [
    "checkpoints, run_args = get_model_checkpoints(folder)\n",
    "if os.name == 'nt':\n",
    "       [print(_.split(\"\\\\\")[1]) for _ in checkpoints];\n",
    "else:\n",
    "       [print(_.split(\"/\")[1]) for _ in checkpoints];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ RUNNING  /home/asebaq/MSMatch/trained_models/ms/50/fixmatch/eurosat_ms/FixMatch_archefficientnet-b0_batch64_confidence0.95_lr0.03_uratio7_wd0.0005_wu1.0_seed0_numlabels50_optSGD/model_best.pth  -----------------\n",
      "{'dataset': 'eurosat_ms', 'net': 'efficientnet-b0', 'batch': 64, 'confidence': 0.95, 'lr': 0.03, 'uratio': 7, 'wd': 0.0005, 'wu': 1.0, 'seed': 0, 'numlabels': 50, 'opt': 'SGD', 'iterations': 567000}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for checkpoint, args in zip(checkpoints,run_args):\n",
    "    print(\"------------ RUNNING \", checkpoint, \" -----------------\")\n",
    "    print(args)\n",
    "    args[\"batch_size\"] = 256\n",
    "    args[\"data_dir\"] = \"/home/asebaq/MSMatch/data/\"\n",
    "    args[\"use_train_model\"] = False\n",
    "    args[\"load_path\"] = checkpoint\n",
    "    if args[\"seed\"] == seed_wanted:\n",
    "        checkpoint_path = os.path.join(args[\"load_path\"])\n",
    "        checkpoint = torch.load(checkpoint_path,map_location='cuda:0')\n",
    "        load_model = (checkpoint[\"train_model\"] if args[\"use_train_model\"] else checkpoint[\"eval_model\"])\n",
    "        _net_builder = net_builder(args[\"net\"],False,{})\n",
    "        _eval_dset = SSL_Dataset(name=args[\"dataset\"], train=False, data_dir=args[\"data_dir\"], seed=args[\"seed\"])\n",
    "        eval_dset = _eval_dset.get_dset()\n",
    "        net = _net_builder(num_classes=_eval_dset.num_classes, in_channels=_eval_dset.num_channels)\n",
    "        net.load_state_dict(load_model)\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "        net.eval()\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "        eval_loader = get_data_loader(eval_dset, args[\"batch_size\"], num_workers=1)\n",
    "        label_encoding = _eval_dset.label_encoding\n",
    "        inv_transf = _eval_dset.inv_transform\n",
    "    \n",
    "        \n",
    "        print(\"------------ PREDICTING TESTSET -----------------\")\n",
    "        \n",
    "        images, labels, preds = [],[],[]\n",
    "        with torch.no_grad():\n",
    "            for image, target in tqdm(eval_loader):\n",
    "                image = image.type(torch.FloatTensor).cuda()\n",
    "                logit = net(image)\n",
    "                for idx,img in enumerate(image):\n",
    "                    images.append(inv_transf(img.transpose(0,2).cpu().numpy()).transpose(0,2).numpy())\n",
    "                preds.append(logit.cpu().max(1)[1])\n",
    "                labels.append(target)\n",
    "        labels = torch.cat(labels).numpy()\n",
    "        preds = torch.cat(preds).numpy()\n",
    "        test_report = classification_report(labels, preds, target_names=label_encoding, output_dict=True)\n",
    "        test_report[\"params\"] = args\n",
    "        results.append(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     big_df \u001b[38;5;241m=\u001b[39m big_df\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(big_df)\n\u001b[0;32m---> 14\u001b[0m small_df \u001b[38;5;241m=\u001b[39m \u001b[43mclean_results_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbig_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43msort_criterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m small_df\u001b[38;5;241m.\u001b[39mto_csv(csv_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_test_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/MSMatch/utils.py:363\u001b[0m, in \u001b[0;36mclean_results_df\u001b[0;34m(original_df, data_folder_name, sort_criterion, keep_per_class)\u001b[0m\n\u001b[1;32m    350\u001b[0m     new_df \u001b[38;5;241m=\u001b[39m original_df\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m    351\u001b[0m         labels\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    352\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    360\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    361\u001b[0m     )\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39m\u001b[43moriginal_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mucm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    366\u001b[0m         new_df \u001b[38;5;241m=\u001b[39m original_df\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m    367\u001b[0m             labels\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    398\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/msmatch1/lib/python3.9/site-packages/pandas/core/indexes/base.py:5363\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[1;32m   5361\u001b[0m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[1;32m   5362\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mcast_scalar_indexer(key, warn_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 5363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   5366\u001b[0m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[1;32m   5367\u001b[0m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[1;32m   5368\u001b[0m     result \u001b[38;5;241m=\u001b[39m getitem(key)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "big_df = pd.DataFrame()\n",
    "pd.set_option('display.max_columns', None)\n",
    "for result in results:\n",
    "    params = result[\"params\"]\n",
    "    df = pd.DataFrame(result)\n",
    "    df.drop(list(params.keys()),inplace=True)\n",
    "    df.drop([\"support\",\"recall\",\"precision\"],inplace=True)\n",
    "    for key,val in params.items():\n",
    "        df[key] = val\n",
    "    df = df.set_index(\"dataset\")\n",
    "    big_df = big_df.append(df)\n",
    "print(big_df)\n",
    "small_df = clean_results_df(big_df, folder,sort_criterion)\n",
    "small_df.to_csv(csv_folder + \"_test_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b3978d1cb0bd70de915a89505f2200e7652bb1fca6aa7e55aa30b9ad177cab2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
